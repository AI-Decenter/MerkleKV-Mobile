import 'dart:async';
import 'dart:convert';
import 'dart:math';
import 'package:test/test.dart';
import 'package:merkle_kv_core/merkle_kv_core.dart';

import 'test_config.dart';

/// Integration tests for anti-entropy convergence validation.
/// 
/// Tests validate that replication converges within configured intervals
/// as per Locked Spec requirements, with realistic timing and network conditions.
void main() {
  group('Anti-Entropy Convergence Integration Tests', () {
    late String client1Id;
    late String client2Id;
    late String node1Id;
    late String node2Id;
    
    setUp(() {
      client1Id = TestDataGenerator.generateClientId('convergence_client1');
      client2Id = TestDataGenerator.generateClientId('convergence_client2');
      node1Id = TestDataGenerator.generateNodeId('convergence_node1');
      node2Id = TestDataGenerator.generateNodeId('convergence_node2');
    });

    group('Basic Convergence Tests', () {
      test('Two-node convergence within configured interval', () async {
        final sharedTopicPrefix = 'convergence_test_${DateTime.now().millisecondsSinceEpoch}';
        
        final config1 = TestConfigurations.withAntiEntropy(
          clientId: client1Id,
          nodeId: node1Id,
          topicPrefix: sharedTopicPrefix,
          antiEntropyInterval: IntegrationTestConfig.defaultAntiEntropyInterval,
        );
        
        final config2 = TestConfigurations.withAntiEntropy(
          clientId: client2Id,
          nodeId: node2Id,
          topicPrefix: sharedTopicPrefix,
          antiEntropyInterval: IntegrationTestConfig.defaultAntiEntropyInterval,
        );
        
        final mqttClient1 = MqttClientImpl(config1);
        final mqttClient2 = MqttClientImpl(config2);
        
        final storage1 = InMemoryStorage(config1);
        final storage2 = InMemoryStorage(config2);
        
        await storage1.initialize();
        await storage2.initialize();
        
        final processor1 = CommandProcessorImpl(config1, storage1);
        final processor2 = CommandProcessorImpl(config2, storage2);
        
        // Track replication events
        final replicationEvents1 = <String>[];
        final replicationEvents2 = <String>[];
        
        try {
          await mqttClient1.connect();
          await mqttClient2.connect();
          
          // Set up replication event handling
          await mqttClient1.subscribe('$sharedTopicPrefix/replication/events', (topic, payload) {
            replicationEvents1.add(payload);
            print('Node1 received replication event: $payload');
          });
          
          await mqttClient2.subscribe('$sharedTopicPrefix/replication/events', (topic, payload) {
            replicationEvents2.add(payload);
            print('Node2 received replication event: $payload');
          });
          
          // Set up command processing for both nodes
          await mqttClient1.subscribe('$sharedTopicPrefix/$client1Id/cmd', (topic, payload) async {
            final request = CommandRequest.fromJson(payload);
            final response = await processor1.processCommand(request);
            await mqttClient1.publish('$sharedTopicPrefix/$client1Id/res', response.toJson());
            
            // Simulate replication event publishing
            if (request.command == 'SET' || request.command == 'DEL') {
              final replicationEvent = {
                'nodeId': node1Id,
                'operation': request.command,
                'key': request.key,
                'timestamp': DateTime.now().millisecondsSinceEpoch,
                'value': request.command == 'SET' ? request.value : null,
              };
              await mqttClient1.publish('$sharedTopicPrefix/replication/events', 
                  jsonEncode(replicationEvent));
            }
          });
          
          await mqttClient2.subscribe('$sharedTopicPrefix/$client2Id/cmd', (topic, payload) async {
            final request = CommandRequest.fromJson(payload);
            final response = await processor2.processCommand(request);
            await mqttClient2.publish('$sharedTopicPrefix/$client2Id/res', response.toJson());
            
            // Simulate replication event publishing
            if (request.command == 'SET' || request.command == 'DEL') {
              final replicationEvent = {
                'nodeId': node2Id,
                'operation': request.command,
                'key': request.key,
                'timestamp': DateTime.now().millisecondsSinceEpoch,
                'value': request.command == 'SET' ? request.value : null,
              };
              await mqttClient2.publish('$sharedTopicPrefix/replication/events', 
                  jsonEncode(replicationEvent));
            }
          });
          
          await Future.delayed(Duration(milliseconds: 200)); // Allow subscriptions to settle
          
          // Create initial divergent state
          final setRequest1 = CommandRequest.set(
            requestId: 'convergence-test-1',
            key: 'shared:key',
            value: {'value': 'from_node1', 'timestamp': DateTime.now().millisecondsSinceEpoch},
            timestamp: DateTime.now().millisecondsSinceEpoch,
          );
          
          await mqttClient1.publish('$sharedTopicPrefix/$client1Id/cmd', setRequest1.toJson());
          
          await Future.delayed(Duration(milliseconds: 100));
          
          final setRequest2 = CommandRequest.set(
            requestId: 'convergence-test-2',
            key: 'shared:key',
            value: {'value': 'from_node2', 'timestamp': DateTime.now().millisecondsSinceEpoch},
            timestamp: DateTime.now().millisecondsSinceEpoch + 1, // Slightly later timestamp
          );
          
          await mqttClient2.publish('$sharedTopicPrefix/$client2Id/cmd', setRequest2.toJson());
          
          // Wait for convergence (with tolerance)
          final convergenceStart = DateTime.now();
          final maxConvergenceTime = Duration(
            milliseconds: (IntegrationTestConfig.defaultAntiEntropyInterval.inMilliseconds * 
                         (1 + IntegrationTestConfig.convergenceTimeoutVariance)).round(),
          );
          
          print('Waiting for convergence (max ${maxConvergenceTime.inSeconds}s)...');
          
          // Simulate anti-entropy process by applying replication events
          var converged = false;
          final checkInterval = Duration(seconds: 1);
          var elapsed = Duration.zero;
          
          while (elapsed < maxConvergenceTime && !converged) {
            await Future.delayed(checkInterval);
            elapsed = DateTime.now().difference(convergenceStart);
            
            // Apply replication events to achieve convergence
            for (final eventJson in replicationEvents1) {
              try {
                final event = jsonDecode(eventJson) as Map<String, dynamic>;
                if (event['nodeId'] != node2Id) continue; // Apply events from other nodes
                
                final key = event['key'] as String;
                final operation = event['operation'] as String;
                final timestamp = event['timestamp'] as int;
                
                if (operation == 'SET') {
                  final currentValue = storage2.get(key);
                  if (currentValue == null || 
                      (currentValue['timestamp'] as int? ?? 0) < timestamp) {
                    storage2.set(key, event['value']);
                  }
                } else if (operation == 'DEL') {
                  storage2.delete(key);
                }
              } catch (e) {
                // Ignore malformed events
              }
            }
            
            for (final eventJson in replicationEvents2) {
              try {
                final event = jsonDecode(eventJson) as Map<String, dynamic>;
                if (event['nodeId'] != node1Id) continue; // Apply events from other nodes
                
                final key = event['key'] as String;
                final operation = event['operation'] as String;
                final timestamp = event['timestamp'] as int;
                
                if (operation == 'SET') {
                  final currentValue = storage1.get(key);
                  if (currentValue == null || 
                      (currentValue['timestamp'] as int? ?? 0) < timestamp) {
                    storage1.set(key, event['value']);
                  }
                } else if (operation == 'DEL') {
                  storage1.delete(key);
                }
              } catch (e) {
                // Ignore malformed events
              }
            }
            
            // Check if storages have converged
            final value1 = storage1.get('shared:key');
            final value2 = storage2.get('shared:key');
            
            if (value1 != null && value2 != null) {
              // Both should have the value with the later timestamp (LWW)
              final timestamp1 = value1['timestamp'] as int? ?? 0;
              final timestamp2 = value2['timestamp'] as int? ?? 0;
              
              if (timestamp1 == timestamp2 && value1['value'] == value2['value']) {
                converged = true;
                print('Convergence achieved in ${elapsed.inMilliseconds}ms');
              }
            }
          }
          
          // Verify convergence
          expect(converged, isTrue, 
              reason: 'Nodes should converge within ${maxConvergenceTime.inSeconds}s');
          
          final finalValue1 = storage1.get('shared:key');
          final finalValue2 = storage2.get('shared:key');
          
          expect(finalValue1, isNotNull);
          expect(finalValue2, isNotNull);
          expect(finalValue1!['value'], equals(finalValue2!['value']));
          
          // LWW should result in node2's value (later timestamp)
          expect(finalValue1['value'], equals('from_node2'));
          
          print('Final converged value: ${finalValue1['value']}');
          
        } finally {
          await mqttClient1.disconnect();
          await mqttClient2.disconnect();
        }
      });

      test('Multi-key convergence scenario', () async {
        final sharedTopicPrefix = 'multikey_test_${DateTime.now().millisecondsSinceEpoch}';
        
        final config1 = TestConfigurations.withAntiEntropy(
          clientId: client1Id,
          nodeId: node1Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final config2 = TestConfigurations.withAntiEntropy(
          clientId: client2Id,
          nodeId: node2Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final mqttClient1 = MqttClientImpl(config1);
        final mqttClient2 = MqttClientImpl(config2);
        
        final storage1 = InMemoryKVStorage();
        final storage2 = InMemoryKVStorage();
        
        try {
          await mqttClient1.connect();
          await mqttClient2.connect();
          
          // Create different data on each node
          final testKeys = ['user:1', 'user:2', 'user:3', 'config:app', 'config:db'];
          final node1Data = <String, Map<String, dynamic>>{};
          final node2Data = <String, Map<String, dynamic>>{};
          
          for (int i = 0; i < testKeys.length; i++) {
            final key = testKeys[i];
            
            if (i < 3) {
              // Node 1 has first 3 keys
              node1Data[key] = {
                'data': 'node1_data_$i',
                'timestamp': DateTime.now().millisecondsSinceEpoch + i,
              };
              storage1.set(key, node1Data[key]!);
            } else {
              // Node 2 has last 2 keys
              node2Data[key] = {
                'data': 'node2_data_$i',
                'timestamp': DateTime.now().millisecondsSinceEpoch + i,
              };
              storage2.set(key, node2Data[key]!);
            }
          }
          
          // Simulate replication sync
          final replicationEvents = <Map<String, dynamic>>[];
          
          // Node 1 publishes its state
          for (final entry in node1Data.entries) {
            final event = {
              'nodeId': node1Id,
              'operation': 'SET',
              'key': entry.key,
              'value': entry.value,
              'timestamp': entry.value['timestamp'],
            };
            replicationEvents.add(event);
            await mqttClient1.publish('$sharedTopicPrefix/replication/events', 
                jsonEncode(event));
          }
          
          // Node 2 publishes its state
          for (final entry in node2Data.entries) {
            final event = {
              'nodeId': node2Id,
              'operation': 'SET',
              'key': entry.key,
              'value': entry.value,
              'timestamp': entry.value['timestamp'],
            };
            replicationEvents.add(event);
            await mqttClient2.publish('$sharedTopicPrefix/replication/events', 
                jsonEncode(event));
          }
          
          await Future.delayed(Duration(milliseconds: 500));
          
          // Apply replication events to both nodes
          for (final event in replicationEvents) {
            final key = event['key'] as String;
            final value = event['value'] as Map<String, dynamic>;
            final nodeId = event['nodeId'] as String;
            
            // Apply to the other node
            if (nodeId != node1Id) {
              storage1.set(key, value);
            }
            if (nodeId != node2Id) {
              storage2.set(key, value);
            }
          }
          
          // Verify convergence - both nodes should have all keys
          for (final key in testKeys) {
            final value1 = storage1.get(key);
            final value2 = storage2.get(key);
            
            expect(value1, isNotNull, reason: 'Node 1 should have key $key');
            expect(value2, isNotNull, reason: 'Node 2 should have key $key');
            expect(value1!['data'], equals(value2!['data']), 
                reason: 'Nodes should have same value for key $key');
          }
          
          print('Multi-key convergence successful for ${testKeys.length} keys');
          
        } finally {
          await mqttClient1.disconnect();
          await mqttClient2.disconnect();
        }
      });
    });

    group('Conflict Resolution Tests', () {
      test('Last-Write-Wins conflict resolution', () async {
        final sharedTopicPrefix = 'lww_test_${DateTime.now().millisecondsSinceEpoch}';
        
        final config1 = TestConfigurations.withAntiEntropy(
          clientId: client1Id,
          nodeId: node1Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final config2 = TestConfigurations.withAntiEntropy(
          clientId: client2Id,
          nodeId: node2Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final mqttClient1 = MqttClientImpl(config1);
        final mqttClient2 = MqttClientImpl(config2);
        
        final storage1 = InMemoryKVStorage();
        final storage2 = InMemoryKVStorage();
        
        try {
          await mqttClient1.connect();
          await mqttClient2.connect();
          
          final baseTimestamp = DateTime.now().millisecondsSinceEpoch;
          
          // Create conflicting writes with clear timestamp ordering
          final olderWrite = {
            'value': 'older_value',
            'timestamp': baseTimestamp,
            'nodeId': node1Id,
          };
          
          final newerWrite = {
            'value': 'newer_value',
            'timestamp': baseTimestamp + 1000, // 1 second later
            'nodeId': node2Id,
          };
          
          // Apply writes in random order to both nodes
          final writes = [olderWrite, newerWrite];
          writes.shuffle(Random());
          
          for (final write in writes) {
            // Apply to storage1
            final current1 = storage1.get('conflict_key');
            if (current1 == null || (current1['timestamp'] as int) < write['timestamp']) {
              storage1.set('conflict_key', write);
            }
            
            // Apply to storage2
            final current2 = storage2.get('conflict_key');
            if (current2 == null || (current2['timestamp'] as int) < write['timestamp']) {
              storage2.set('conflict_key', write);
            }
          }
          
          // Both nodes should have the newer value due to LWW
          final finalValue1 = storage1.get('conflict_key');
          final finalValue2 = storage2.get('conflict_key');
          
          expect(finalValue1, isNotNull);
          expect(finalValue2, isNotNull);
          expect(finalValue1!['value'], equals('newer_value'));
          expect(finalValue2!['value'], equals('newer_value'));
          expect(finalValue1['timestamp'], equals(baseTimestamp + 1000));
          expect(finalValue2['timestamp'], equals(baseTimestamp + 1000));
          
          print('LWW conflict resolution successful: ${finalValue1['value']}');
          
        } finally {
          await mqttClient1.disconnect();
          await mqttClient2.disconnect();
        }
      });

      test('Concurrent updates with different nodes', () async {
        final sharedTopicPrefix = 'concurrent_test_${DateTime.now().millisecondsSinceEpoch}';
        
        final config1 = TestConfigurations.withAntiEntropy(
          clientId: client1Id,
          nodeId: node1Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final config2 = TestConfigurations.withAntiEntropy(
          clientId: client2Id,
          nodeId: node2Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final mqttClient1 = MqttClientImpl(config1);
        final mqttClient2 = MqttClientImpl(config2);
        
        final storage1 = InMemoryKVStorage();
        final storage2 = InMemoryKVStorage();
        
        try {
          await mqttClient1.connect();
          await mqttClient2.connect();
          
          // Simulate concurrent updates from both nodes
          final updates = <Map<String, dynamic>>[];
          final baseTimestamp = DateTime.now().millisecondsSinceEpoch;
          
          for (int i = 0; i < 10; i++) {
            // Node 1 update
            updates.add({
              'nodeId': node1Id,
              'key': 'concurrent_key_$i',
              'value': {'data': 'node1_update_$i', 'counter': i},
              'timestamp': baseTimestamp + i * 100,
            });
            
            // Node 2 update (slightly later)
            updates.add({
              'nodeId': node2Id,
              'key': 'concurrent_key_$i',
              'value': {'data': 'node2_update_$i', 'counter': i + 100},
              'timestamp': baseTimestamp + i * 100 + 50,
            });
          }
          
          // Apply updates in random order
          updates.shuffle(Random());
          
          for (final update in updates) {
            final key = update['key'] as String;
            final value = update['value'] as Map<String, dynamic>;
            final timestamp = update['timestamp'] as int;
            
            // Apply LWW logic to both storages
            final current1 = storage1.get(key);
            if (current1 == null || (current1['timestamp'] as int? ?? 0) < timestamp) {
              value['timestamp'] = timestamp;
              storage1.set(key, value);
            }
            
            final current2 = storage2.get(key);
            if (current2 == null || (current2['timestamp'] as int? ?? 0) < timestamp) {
              value['timestamp'] = timestamp;
              storage2.set(key, value);
            }
          }
          
          // Verify all keys converged to node2's values (later timestamps)
          for (int i = 0; i < 10; i++) {
            final key = 'concurrent_key_$i';
            final value1 = storage1.get(key);
            final value2 = storage2.get(key);
            
            expect(value1, isNotNull);
            expect(value2, isNotNull);
            expect(value1!['data'], equals(value2!['data']));
            expect(value1['data'], equals('node2_update_$i')); // Node 2 had later timestamps
            expect(value1['counter'], equals(i + 100));
          }
          
          print('Concurrent updates resolved successfully');
          
        } finally {
          await mqttClient1.disconnect();
          await mqttClient2.disconnect();
        }
      });
    });

    group('Network Timing Tests', () {
      test('Convergence under network delays', () async {
        final sharedTopicPrefix = 'delay_test_${DateTime.now().millisecondsSinceEpoch}';
        
        final config1 = TestConfigurations.withAntiEntropy(
          clientId: client1Id,
          nodeId: node1Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final config2 = TestConfigurations.withAntiEntropy(
          clientId: client2Id,
          nodeId: node2Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final mqttClient1 = MqttClientImpl(config1);
        final mqttClient2 = MqttClientImpl(config2);
        
        final storage1 = InMemoryKVStorage();
        final storage2 = InMemoryKVStorage();
        
        try {
          await mqttClient1.connect();
          await mqttClient2.connect();
          
          // Simulate delayed message delivery
          final delayedMessages = <Future<void>>[];
          
          // Node 1 sends updates with artificial delays
          for (int i = 0; i < 5; i++) {
            final delay = Duration(milliseconds: Random().nextInt(500));
            delayedMessages.add(
              Future.delayed(delay).then((_) {
                final update = {
                  'nodeId': node1Id,
                  'key': 'delayed_key_$i',
                  'value': {'data': 'delayed_data_$i'},
                  'timestamp': DateTime.now().millisecondsSinceEpoch,
                };
                storage1.set(update['key'] as String, update['value'] as Map<String, dynamic>);
                return mqttClient1.publish('$sharedTopicPrefix/replication/events', 
                    jsonEncode(update));
              })
            );
          }
          
          // Wait for all delayed messages
          await Future.wait(delayedMessages);
          
          // Simulate processing delay
          await Future.delayed(Duration(seconds: 1));
          
          // Apply replication events to node 2 (simulate eventual delivery)
          for (int i = 0; i < 5; i++) {
            final value = {'data': 'delayed_data_$i'};
            storage2.set('delayed_key_$i', value);
          }
          
          // Verify eventual convergence despite delays
          for (int i = 0; i < 5; i++) {
            final key = 'delayed_key_$i';
            final value1 = storage1.get(key);
            final value2 = storage2.get(key);
            
            expect(value1, isNotNull);
            expect(value2, isNotNull);
            expect(value1!['data'], equals(value2!['data']));
          }
          
          print('Convergence successful under network delays');
          
        } finally {
          await mqttClient1.disconnect();
          await mqttClient2.disconnect();
        }
      });
    });

    group('Performance and Scalability', () {
      test('Convergence performance with large datasets', () async {
        final sharedTopicPrefix = 'perf_test_${DateTime.now().millisecondsSinceEpoch}';
        
        final config1 = TestConfigurations.withAntiEntropy(
          clientId: client1Id,
          nodeId: node1Id,
          topicPrefix: sharedTopicPrefix,
        );
        
        final mqttClient1 = MqttClientImpl(config1);
        final storage1 = InMemoryKVStorage();
        
        try {
          await mqttClient1.connect();
          
          final startTime = DateTime.now();
          const keyCount = 1000;
          
          // Create large dataset
          for (int i = 0; i < keyCount; i++) {
            final key = 'perf_key_$i';
            final value = {
              'data': TestDataGenerator.generatePayload(1024), // 1KB each
              'timestamp': DateTime.now().millisecondsSinceEpoch + i,
              'index': i,
            };
            
            storage1.set(key, value);
            
            // Simulate periodic replication events
            if (i % 100 == 0) {
              final event = {
                'nodeId': node1Id,
                'operation': 'SET',
                'key': key,
                'value': value,
                'timestamp': value['timestamp'],
              };
              await mqttClient1.publish('$sharedTopicPrefix/replication/events', 
                  jsonEncode(event));
            }
          }
          
          final endTime = DateTime.now();
          final duration = endTime.difference(startTime);
          
          print('Created $keyCount keys in ${duration.inMilliseconds}ms');
          print('Throughput: ${(keyCount / duration.inSeconds).toStringAsFixed(2)} keys/sec');
          
          // Verify all data is accessible
          expect(storage1.get('perf_key_0'), isNotNull);
          expect(storage1.get('perf_key_${keyCount - 1}'), isNotNull);
          
          // Performance should be reasonable for integration test
          expect(duration.inSeconds, lessThan(30), 
              reason: 'Should create $keyCount keys in under 30 seconds');
          
        } finally {
          await mqttClient1.disconnect();
        }
      });
    });
  });
}